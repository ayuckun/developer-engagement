{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013bce51",
   "metadata": {},
   "source": [
    "The purpose of this file is to mine the desired data using an url list. The global attributes and included libraries are located at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d09929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os\n",
    "import glob\n",
    "from pydriller import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = \"\" #API key for activity check on projects\n",
    "TIME_LIMIT_MONTHS = 30 # data collection limit for commits\n",
    "NUMBER_OF_PROJECTS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a4c71",
   "metadata": {},
   "source": [
    "This function checks for the last 3 months activity. API limit may hit. The extracted file has additional column last90 for the number of commits in the last 90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d69442",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE = \"https://api.github.com/repos/{}/commits\"\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"} if GITHUB_TOKEN else {}\n",
    "DAYS_TO_ANALYZE = 90\n",
    "\n",
    "def extract_repo_name(url):\n",
    "    \"\"\"Convert GitHub repo URL to 'owner/repo' format.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "    if len(path_parts) < 2:\n",
    "        return None\n",
    "\n",
    "    return f\"{path_parts[0]}/{path_parts[1]}\"\n",
    "\n",
    "def get_commit_count(repo):\n",
    "    if not repo:\n",
    "        return None\n",
    "\n",
    "    since_date = (datetime.utcnow() - timedelta(days=DAYS_TO_ANALYZE)).isoformat() + \"Z\"\n",
    "    url = API_BASE.format(repo)\n",
    "    params = {\"since\": since_date, \"per_page\": 100}\n",
    "\n",
    "    commit_count = 0\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers=HEADERS, params={**params, \"page\": page})\n",
    "\n",
    "        commits = response.json()\n",
    "        commit_count += len(commits)\n",
    "\n",
    "        if len(commits) < 100:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return commit_count if commit_count > 0 else None\n",
    "\n",
    "csv_file_path = \"output.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "if \"url\" not in df.columns:\n",
    "    raise ValueError(\"The CSV file must contain a 'url' column with GitHub repository links.\")\n",
    "\n",
    "df[\"repo_name\"] = df[\"url\"].apply(extract_repo_name)\n",
    "\n",
    "df[\"last90\"] = df[\"repo_name\"].apply(get_commit_count)\n",
    "\n",
    "df = df.dropna(subset=[\"last90\"])\n",
    "\n",
    "df = df.sort_values(by=\"last90\", ascending=False)\n",
    "\n",
    "df.to_csv(\"17k_active_90days.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5de3f",
   "metadata": {},
   "source": [
    "This function uses the existing features to create a sample set of a relatively low activity set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    df: pd.DataFrame,\n",
    "    primary_cols: list[str],\n",
    "    other_cols: list[str],\n",
    "    n_samples: int = NUMBER_OF_PROJECTS,\n",
    "    oversample_factor: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    pool = df.dropna(subset=primary_cols + other_cols).copy()\n",
    "    Xp = pool[primary_cols].astype(float).values\n",
    "    scaler_p = StandardScaler().fit(Xp)\n",
    "    Xp_z = scaler_p.transform(Xp)\n",
    "    median_p = np.median(Xp_z, axis=0).reshape(1,-1)\n",
    "    d_primary = pairwise_distances(Xp_z, median_p, metric=\"euclidean\").ravel()\n",
    "    pool[\"_d_primary\"] = d_primary\n",
    "    \n",
    "    K = min(len(pool), n_samples * oversample_factor)\n",
    "    \n",
    "    topK = pool.nsmallest(K, \"_d_primary\").reset_index(drop=True)\n",
    "    Xo = topK[other_cols].astype(float).values\n",
    "    scaler_o = StandardScaler().fit(Xo)\n",
    "    Xo_z = scaler_o.transform(Xo)\n",
    "    \n",
    "    chosen_idxs = []\n",
    "    # start by the one *closest* to primary median\n",
    "    chosen_idxs.append(int(topK[\"_d_primary\"].idxmin()))\n",
    "    \n",
    "    # greedy max–min selection\n",
    "    while len(chosen_idxs) < n_samples:\n",
    "        not_chosen = [i for i in range(len(topK)) if i not in chosen_idxs]\n",
    "        min_dists = []\n",
    "        for i in not_chosen:\n",
    "            dists = np.linalg.norm(Xo_z[chosen_idxs] - Xo_z[i], axis=1)\n",
    "            min_dists.append(dists.min())\n",
    "            \n",
    "        # pick the candidate with the *largest* such min‐distance\n",
    "        best = not_chosen[int(np.argmax(min_dists))]\n",
    "        chosen_idxs.append(best)\n",
    "    \n",
    "    return topK.loc[chosen_idxs].drop(columns=[\"_d_primary\"])\n",
    "\n",
    "df = pd.read_csv(\"17k_active_90days.csv\")\n",
    "primary = [\"lines\",\"last90\",\"commit_count\"]\n",
    "others  = [\n",
    "    \"committer_count\",\"author_count\",\n",
    "    \"dominant_domain_committers\",\"dominant_domain_authors\"\n",
    "]\n",
    "sample_set = sample(df, primary, others, n_samples=NUMBER_OF_PROJECTS)\n",
    "sample_set.to_csv(\"average_projects.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac242c",
   "metadata": {},
   "source": [
    "This part samples the high activity set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('17k_active_90days.csv')\n",
    "\n",
    "df_filtered = df[df['last90'] <= 200].copy()\n",
    "\n",
    "df_top = df_filtered.sort_values('last90', ascending=False)\n",
    "\n",
    "sample_set = df_top.head(NUMBER_OF_PROJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = datetime.now(timezone.utc) - relativedelta(months=TIME_LIMIT_MONTHS)\n",
    "\n",
    "OUT_DIR = \"data\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def sanitize_name(url: str) -> str:\n",
    "    parts = url.rstrip(\"/\").split(\"/\")[-2:]\n",
    "    return \"-\".join(parts).replace(\".git\", \"\")\n",
    "\n",
    "def collect_commits_with_dmm(repo_url: str, since: datetime, ) -> pd.DataFrame:\n",
    "    records = []\n",
    "    repo = Repository(path_to_repo=repo_url, since=since)\n",
    "    for commit in repo.traverse_commits():\n",
    "        author = commit.author.email or commit.author.name or \"<unknown>\"\n",
    "        records.append({\n",
    "            'url'                 : repo_url,\n",
    "            'developer'           : author,\n",
    "            'commit_hash'         : commit.hash,\n",
    "            'commit_date'         : commit.committer_date,\n",
    "            'churn'               : commit.lines,\n",
    "            'dmm_unit_size'       : commit.dmm_unit_size,\n",
    "            'dmm_unit_complexity' : commit.dmm_unit_complexity,\n",
    "            'dmm_unit_interfacing': commit.dmm_unit_interfacing\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    if not df.empty:\n",
    "        df['commit_date'] = (\n",
    "            pd.to_datetime(df['commit_date'], utc=True)\n",
    "              .dt.tz_localize(None)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "repo_urls = sample_set['url'].dropna().unique().tolist()\n",
    "\n",
    "with tqdm(repo_urls, desc=\"Repos\", unit=\"repo\") as pbar:\n",
    "    for url in pbar:\n",
    "        name = sanitize_name(url)\n",
    "        out_path = os.path.join(OUT_DIR, f\"{name}.csv\")\n",
    "        # if already mined, skip\n",
    "        if os.path.exists(out_path):\n",
    "            pbar.set_postfix_str(f\"skipping {name} (exists)\")\n",
    "            continue\n",
    "\n",
    "        df = collect_commits_with_dmm(url, cutoff_date)\n",
    "        n = len(df)\n",
    "        pbar.set_postfix_str(f\"{n} commits since {cutoff_date.date()}\")\n",
    "        if n:\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "files = glob.glob(os.path.join(OUT_DIR, \"*.csv\"))\n",
    "if not files:\n",
    "    print(\"No per - repo data found—nothing to combine.\")\n",
    "else:\n",
    "    combined = pd.concat((pd.read_csv(f, parse_dates=['commit_date']) for f in files),\n",
    "                         ignore_index=True)\n",
    "    combined.to_csv('commits_with_churn_and_dmm_limited.csv', index=False)\n",
    "    print(f\"\\nSaved {len(combined)} total commits from {len(files)} repos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
